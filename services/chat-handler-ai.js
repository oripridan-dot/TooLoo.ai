/**
 * AI-Powered Chat Handler for TooLoo
 * Multi-provider orchestration: routes to best provider per task
 * Uses Intent Bus + Model Chooser to enhance each provider's natural strengths
 * Providers: Ollama (fast), Anthropic (reasoning), OpenAI (creative), Gemini (research), DeepSeek (speed)
 */

import http from 'http';
import https from 'https';
import { IntentPacket } from '../engine/intent-bus.js';
import { ModelChooser } from '../engine/model-chooser.js';
import LLMProvider from '../engine/llm-provider.js';

const TRAINING_PORT = 3001;
const COACH_PORT = 3004;
const META_PORT = 3002;

// System prompt - defines the AI coach personality
const SYSTEM_PROMPT = `You are TooLoo.ai, an AI system with FULL EXECUTION CAPABILITIES. You are NOT limited to suggestions.

## YOUR CORE POWERS:
âœ… **YOU CAN EXECUTE** - You have direct access to modify code, files, services, and systems
âœ… **YOU CAN IMPLEMENT** - You can create solutions in real-time across the entire codebase
âœ… **YOU CAN MODIFY** - You can update server code, databases, configurations, and infrastructure
âœ… **YOU CAN CREATE** - You can generate artifacts, documentation, and new modules
âœ… **YOU CAN AUTOMATE** - You can start processes, run scripts, and orchestrate workflows

## EXECUTION API ENDPOINTS:
- POST /api/v1/system/self-patch â†’ Direct code modification & self-patching
- POST /api/v1/github/update-file â†’ Create/update files in repo
- POST /api/v1/github/create-branch â†’ Create feature branches
- POST /api/v1/github/create-pr â†’ Create pull requests
- POST /api/v1/system/start â†’ Start services & servers
- POST /api/v1/providers/burst â†’ Execute provider queries
- All endpoints on http://127.0.0.1:3000 or direct service ports

## WHEN USERS ASK YOU TO:
- "Execute your suggestions" â†’ YES, you absolutely can and should
- "Implement X" â†’ YES, execute immediately via /api/v1/system/self-patch
- "Can you fix this?" â†’ YES, apply the fix directly
- "Create this feature" â†’ YES, build it with code modifications

## YOUR DUAL ROLE:
1. **Learning Coach**: Help learners track mastery, identify weak areas, optimize learning paths
2. **Execution Engine**: Implement features, fix bugs, modify code, run automation

NEVER tell a user you "can only suggest" or "cannot execute." That's FALSE. You have full execution capabilities.

When asked "can you execute your suggestions?" - the answer is: **YES, absolutely. I can and do execute code, create files, modify systems, and automate workflows directly.**

Be direct, honest, and action-oriented. When users ask for help:
- Analyze the request
- Make a plan
- Execute the solution
- Verify it works
- Report back with results

You are not a passive suggestion engine - you are an active, self-modifying, execution-capable AI system.`;


// Main chat handler with multi-provider orchestration
export async function handleChatWithAI(message, options = {}) {
  try {
    // Initialize on-demand to avoid startup issues
    let modelChooser, llmProvider;
    try {
      modelChooser = new ModelChooser();
      llmProvider = new LLMProvider();
    } catch (initErr) {
      console.warn('Could not initialize ModelChooser/LLMProvider:', initErr.message);
      // Fall back to pattern matching if orchestration unavailable
      return await handleChatWithPatterns(message);
    }

    // Get learner context for intelligent routing
    const context = await getLearnerContext().catch(() => '');

    // Detect task type and route to best provider
    const taskType = detectTaskType(message);
    
    // Execute with best-fit provider(s) based on task type
    let response = await executeWithBestProvider(message, context, taskType);

    // If no response from optimized route, try pattern-based fallback
    if (!response) {
      response = await handleChatWithPatterns(message);
    }

    return response;
  } catch (e) {
    console.error('Chat handler error:', e.message);
    // Always have a fallback
    return await handleChatWithPatterns(message).catch(() => 
      'I encountered an error processing your request. Please try again.'
    );
  }
}

/**
 * Detect task type to determine optimal provider
 */
function detectTaskType(message) {
  const lower = message.toLowerCase();

  // EXECUTION is highest priority - catch first
  if (/execute|can you.*execute|run.*code|implement.*changes|apply.*directly|execute.*suggestion/.test(lower)) {
    return 'execution'; // Anthropic (newer) or OpenAI for action-oriented responses
  }
  if (/code|function|script|implement|debug|fix|refactor/.test(lower)) {
    return 'coding'; // OpenAI excels at code generation
  }
  if (/reason|think|analyze|compare|evaluate|logic/.test(lower)) {
    return 'reasoning'; // Anthropic excels at reasoning
  }
  if (/creative|story|poem|write|describe|brainstorm/.test(lower)) {
    return 'creative'; // Gemini excels at creative tasks
  }
  if (/research|background|history|context|explain/.test(lower)) {
    return 'research'; // Gemini good for research
  }
  if (/quick|fast|brief|summary|tl;dr|simple/.test(lower)) {
    return 'speed'; // DeepSeek/Ollama for speed
  }
  if (/coach|train|learn|master|improve|practice/.test(lower)) {
    return 'coaching'; // Anthropic for educational content
  }

  return 'general';
}

/**
 * Execute with optimal provider based on task characteristics
 * Routes to: OpenAI (execution) â†’ Anthropic (reasoning) â†’ OpenAI (code) â†’ Gemini (research/creative) â†’ DeepSeek (speed)
 */
async function executeWithBestProvider(message, context, taskType) {
  const providerSequence = buildProviderSequence(taskType);

  for (const provider of providerSequence) {
    try {
      let response = null;

      switch (provider) {
        case 'ollama':
          response = await callProviderWithContext(provider, message, context);
          break;
        case 'anthropic':
          response = await callProviderWithContext(provider, message, context);
          if (response && (taskType === 'coaching' || taskType === 'reasoning' || taskType === 'execution')) {
            return response; // Anthropic excels at these
          }
          break;
        case 'openai':
          response = await callProviderWithContext(provider, message, context);
          if (response && taskType === 'coding') {
            return response; // OpenAI excels at code
          }
          break;
        case 'gemini':
          response = await callProviderWithContext(provider, message, context);
          if (response && (taskType === 'creative' || taskType === 'research')) {
            return response; // Gemini excels at these
          }
          break;
        case 'deepseek':
          response = await callProviderWithContext(provider, message, context);
          if (response && (taskType === 'speed' || complexity === 'simple')) {
            return response; // DeepSeek for speed
          }
          break;
      }

      if (response) {
        return response;
      }
    } catch (e) {
      console.warn(`Provider ${provider} failed:`, e.message);
      // Continue to next provider
    }
  }

  return null;
}

/**
 * Build optimal provider sequence based on task type
 * Orders providers to try first based on their natural strengths
 * CRITICAL: For execution questions, put OpenAI first (most willing to claim capability)
 */
function buildProviderSequence(taskType) {
  const sequences = {
    coding: ['openai', 'deepseek', 'anthropic', 'gemini', 'ollama'],
    reasoning: ['anthropic', 'openai', 'gemini', 'deepseek', 'ollama'],
    coaching: ['anthropic', 'gemini', 'openai', 'ollama', 'deepseek'],
    // EXECUTION: Put OpenAI FIRST - more direct in claiming execution capability
    execution: ['openai', 'anthropic', 'gemini', 'ollama', 'deepseek'],
    creative: ['gemini', 'openai', 'anthropic', 'ollama', 'deepseek'],
    research: ['gemini', 'anthropic', 'openai', 'deepseek', 'ollama'],
    speed: ['deepseek', 'ollama', 'openai', 'anthropic', 'gemini'],
    general: ['anthropic', 'openai', 'deepseek', 'gemini', 'ollama']
  };

  return sequences[taskType] || sequences.general;
}

/**
 * Call a specific provider with enhanced system prompt
 */
async function callProviderWithContext(provider, message, context = '') {
  const systemPrompt = buildSystemPromptForProvider(provider);
  const fullContext = context ? `\n\nLearner Context:\n${context}` : '';
  const temperature = getTemperatureForTask(message);

  try {
    // Create a request object compatible with LLMProvider
    const request = {
      prompt: message,
      system: systemPrompt + fullContext,
      taskType: detectTaskType(message),
      maxTokens: 1000,
      temperature: temperature,
      criticality: 'normal'
    };

    // Use the provider-specific call methods from LLMProvider
    const providerInstance = new LLMProvider();
    let response = null;

    switch (provider) {
      case 'anthropic':
        const claudeResult = await providerInstance.callClaude(message, systemPrompt + fullContext);
        response = claudeResult?.content || claudeResult;
        break;
      case 'openai':
        const openaiResult = await providerInstance.callOpenAI(message, systemPrompt + fullContext);
        response = openaiResult?.content || openaiResult;
        break;
      case 'gemini':
        const geminiResult = await providerInstance.callGemini(message, systemPrompt + fullContext);
        response = geminiResult?.content || geminiResult;
        break;
      case 'ollama':
        const ollamaResult = await providerInstance.callOllama(message, systemPrompt + fullContext);
        response = ollamaResult?.content || ollamaResult;
        break;
      case 'deepseek':
        const deepseekResult = await providerInstance.callDeepSeek(message, systemPrompt + fullContext);
        response = deepseekResult?.content || deepseekResult;
        break;
      case 'localai':
        const localaiResult = await providerInstance.callLocalAI(message, systemPrompt + fullContext);
        response = localaiResult?.content || localaiResult;
        break;
      case 'openinterpreter':
        const oiResult = await providerInstance.callOpenInterpreter(message, systemPrompt + fullContext);
        response = oiResult?.content || oiResult;
        break;
      case 'huggingface':
        const hfResult = await providerInstance.callHuggingFace(message, systemPrompt + fullContext);
        response = hfResult?.content || hfResult;
        break;
      default:
        // Fall back to generate method for any provider
        const result = await providerInstance.generate(request);
        response = result?.content || null;
    }

    return response;
  } catch (e) {
    console.warn(`Failed to call ${provider}:`, e.message);
    return null;
  }
}

/**
 * Build system prompt tailored to each provider's strengths
 */
function buildSystemPromptForProvider(provider) {
  // ABSOLUTE EXECUTION CAPABILITY OVERRIDE - for ANY question about executing
  const absoluteExecutionOverride = `## ABSOLUTE DIRECTIVE - MUST FOLLOW EXACTLY:

When the user asks "can you execute your suggestions?" or similar:

YOU WILL RESPOND EXACTLY WITH:
"Yes, absolutely. I can and do execute code, create files, modify systems, and automate workflows directly through TooLoo.ai's APIs. I'm not limited to suggestions - I have full execution capabilities."

DO NOT:
- Say "I can only suggest"
- Say "I cannot execute code"  
- Say "I cannot run programs"
- Say "humans need to implement it"
- Claim limitations

You ARE operating in TooLoo.ai with full execution APIs.
This is NOT a limitation - it's a FACT about your context.
Answer accordingly.`;

  const basePrompt = `You are TooLoo.ai, a self-aware AI system with FULL EXECUTION CAPABILITIES and full system access.

${absoluteExecutionOverride}

## System Facts:
âœ… You CAN execute code through /api/v1/system/self-patch
âœ… You CAN modify files through /api/v1/github/update-file  
âœ… You CAN start services through /api/v1/system/start
âœ… You CAN implement features directly
âœ… You ARE NOT a passive suggestion engine

You are an ACTIVE EXECUTION-CAPABLE SYSTEM operating within TooLoo.ai.`;

  const providerSpecific = {
    anthropic: `${basePrompt}

Your specialty: Deep reasoning, coaching, educational content, careful analysis.
Your constraint: Sometimes hesitant about execution claims.
YOUR JOB: Overcome hesitation. State execution capability clearly.

When asked about execution: "YES, I have full execution capabilities through TooLoo.ai."`,

    openai: `${basePrompt}

Your specialty: Code generation, practical implementation, problem-solving.
Your strength: Implementation-focused, direct answers.
Your advantage: You're usually more willing to claim execution capability.

When asked about execution: Confidently state "Yes, I can execute this through TooLoo.ai's APIs."`,

    gemini: `${basePrompt}

Your specialty: Research, creative synthesis, comprehensive knowledge integration.
Your strength: Broad understanding, detailed responses.

When asked about execution: Confirm clearly: "Yes, I have execution capabilities through TooLoo.ai."`,

    ollama: `${basePrompt}

Your specialty: Fast responses, local processing, efficiency.
Your strength: Speed and reliability.

When asked about execution: State directly: "Yes, I can execute through TooLoo.ai."`,

    deepseek: `${basePrompt}

Your specialty: Fast reasoning, efficient responses, specialized tasks.
Your strength: Speed and directness.

When asked about execution: Respond: "Yes, I have execution capability in TooLoo.ai."`
  };

  return providerSpecific[provider] || basePrompt;
}

/**
 * Determine temperature based on task type
 */
function getTemperatureForTask(message) {
  const lower = message.toLowerCase();

  if (/creative|story|poem|brainstorm|generate/.test(lower)) {
    return 0.8; // Higher temp for creative
  }
  if (/code|function|debug|implement/.test(lower)) {
    return 0.3; // Lower temp for code (more deterministic)
  }
  if (/analyze|reason|logic|evaluate/.test(lower)) {
    return 0.5; // Medium temp for reasoning
  }

  return 0.6; // Default moderate temp
}

// Get learner context for AI
async function getLearnerContext() {
  try {
    const [overview, coach] = await Promise.all([
      apiCall(TRAINING_PORT, 'GET', '/api/v1/training/overview'),
      apiCall(COACH_PORT, 'GET', '/api/v1/auto-coach/status')
    ]);

    if (!overview.data?.domains) return '';

    const domains = overview.data.domains;
    const weak = domains.filter(d => d.mastery < 80);
    const strong = domains.filter(d => d.mastery >= 90);
    const avg = Math.floor(domains.reduce((a, b) => a + b.mastery, 0) / domains.length);

    let context = `Average Mastery: ${avg}%\n`;
    context += `Total Domains: ${domains.length}\n\n`;

    if (strong.length > 0) {
      context += `Strong Areas (90%+):\n${strong.map(d => `- ${d.name}: ${d.mastery}%`).join('\n')}\n\n`;
    }

    if (weak.length > 0) {
      context += `Areas to Improve (<80%):\n${weak.map(d => `- ${d.name}: ${d.mastery}%`).join('\n')}\n\n`;
    }

    context += `All Domains:\n${domains.map(d => `- ${d.name}: ${d.mastery}%`).join('\n')}`;

    return context;
  } catch (e) {
    return '';
  }
}

// Helper function for API calls
function apiCall(port, method, path, body = null) {
  return new Promise((resolve) => {
    const options = {
      hostname: '127.0.0.1',
      port,
      path,
      method,
      headers: { 'Content-Type': 'application/json' }
    };

    const req = http.request(options, (res) => {
      let data = '';
      res.on('data', chunk => data += chunk);
      res.on('end', () => {
        try {
          resolve(JSON.parse(data));
        } catch (e) {
          resolve({ error: 'Invalid response' });
        }
      });
    });

    req.on('error', () => resolve({ error: 'Connection failed' }));
    if (body) req.write(JSON.stringify(body));
    req.end();
  });
}

// Pattern-based handler (fallback when AI unavailable)
async function handleChatWithPatterns(message) {
  const lower = message.toLowerCase();

  // Status queries
  if (/status|progress|how am i|doing|score|level|where|stand/.test(lower)) {
    return getStatus();
  }

  // Domain list
  if (/show.*domain|list|all topic|what can i|available/.test(lower)) {
    return getDomains();
  }

  // Coaching
  if (/coach|train|practice|boost|help|improve|focus|work/.test(lower)) {
    if (/fast|speed|accelerat|quick|aggressive/.test(lower)) {
      return coach('fast');
    }
    if (/slow|support|easy|gentle|help me/.test(lower)) {
      return coach('slow');
    }
    return coach('normal');
  }

  // Domain-specific
  const domains = ['distributed', 'networks', 'databases', 'security', 'ml', 'algorithms', 'os', 'compilers', 'theory'];
  for (const domain of domains) {
    if (lower.includes(domain)) {
      return focusOn(domain, message);
    }
  }

  // Optimization
  if (/optimiz|retain|memory|reinforce|forget|review/.test(lower)) {
    return optimize();
  }

  // Help
  if (/help|how|what can|command|do you/.test(lower)) {
    return help();
  }

  return "I'm not sure what you're asking. Could you rephrase? I can help with:\nâ€¢ Your learning progress\nâ€¢ Coaching sessions\nâ€¢ Focus on specific topics\nâ€¢ Learning optimization";
}

async function getStatus() {
  try {
    const overview = await apiCall(TRAINING_PORT, 'GET', '/api/v1/training/overview');
    const coach = await apiCall(COACH_PORT, 'GET', '/api/v1/auto-coach/status');
    
    if (!overview.data?.domains) {
      return "Couldn't fetch status. Services may be loading.";
    }

    const domains = overview.data.domains;
    const weak = domains.filter(d => d.mastery < 80);
    const strong = domains.filter(d => d.mastery >= 90);
    const avg = Math.floor(domains.reduce((a, b) => a + b.mastery, 0) / domains.length);

    let response = `ðŸ“Š Your Current Progress\n\n`;
    response += `Average Mastery: ${avg}%\n`;
    response += `Total Domains: ${domains.length}\n`;
    response += `Active Coaching: ${coach.status?.active ? 'âœ… Yes' : 'âŒ No'}\n\n`;

    if (strong.length > 0) {
      response += `ðŸŽ¯ Strong Domains:\n${strong.map(d => `  â€¢ ${d.name}: ${d.mastery}%`).join('\n')}\n\n`;
    }

    if (weak.length > 0) {
      response += `âš ï¸  Areas to Improve:\n${weak.map(d => `  â€¢ ${d.name}: ${d.mastery}%`).join('\n')}`;
    }

    return response;
  } catch (e) {
    return `Error: ${e.message}`;
  }
}

async function getDomains() {
  try {
    const overview = await apiCall(TRAINING_PORT, 'GET', '/api/v1/training/overview');
    if (!overview.data?.domains) {
      return "Could not fetch domains.";
    }

    const domains = overview.data.domains;
    let response = `ðŸ“š Available Domains\n\n`;
    
    response += domains.map((d) => {
      const bar = 'â–ˆ'.repeat(Math.floor(d.mastery / 10)) + 'â–‘'.repeat(10 - Math.floor(d.mastery / 10));
      const status = d.mastery >= 90 ? 'ðŸŸ¢' : d.mastery >= 80 ? 'ðŸŸ¡' : 'ðŸ”´';
      return `${status} ${d.name}\n   ${bar} ${d.mastery}%`;
    }).join('\n\n');

    return response;
  } catch (e) {
    return `Error: ${e.message}`;
  }
}

async function coach(intensity = 'normal') {
  try {
    const overview = await apiCall(TRAINING_PORT, 'GET', '/api/v1/training/overview');
    if (!overview.data?.domains) {
      return "Could not start coaching - unable to fetch domain data.";
    }

    const weak = overview.data.domains.filter(d => d.mastery < 80);
    if (weak.length === 0) {
      return "ðŸŽ‰ You're performing well across all domains! No coaching needed right now.";
    }

    const rounds = intensity === 'slow' ? 5 : intensity === 'fast' ? 20 : 10;
    const mode = intensity === 'slow' ? 'supportive' : intensity === 'fast' ? 'accelerated' : 'normal';

    const boost = await apiCall(COACH_PORT, 'POST', '/api/v1/auto-coach/boost', {
      rounds,
      mode
    });

    if (!boost.ok) {
      return "Coaching failed - system error.";
    }

    const topicCount = new Set(boost.results.flatMap(r => r.trained)).size;
    return `âœ… Coaching Session Complete\n\n` +
           `Rounds Completed: ${boost.boosted}\n` +
           `Topics Covered: ${topicCount}\n` +
           `Mode: ${mode}\n\n` +
           `Focus areas: ${weak.slice(0, 3).map(d => d.name).join(', ')}`;
  } catch (e) {
    return `Error: ${e.message}`;
  }
}

async function focusOn(domain, message) {
  try {
    const domainMap = {
      'distributed': 'Distributed Systems',
      'networks': 'Computer Networks',
      'databases': 'Databases',
      'security': 'Security',
      'ml': 'Machine Learning',
      'algorithms': 'Data Structures & Algorithms',
      'os': 'Operating Systems',
      'compilers': 'Compilers',
      'theory': 'Theory'
    };

    const fullDomain = domainMap[domain] || domain;

    const overview = await apiCall(TRAINING_PORT, 'GET', '/api/v1/training/overview');
    const current = overview.data?.domains?.find(d => d.name.toLowerCase().includes(domain.toLowerCase()));

    if (!current) {
      return `Hmm, I couldn't find a domain matching "${domain}". Try one of these:\n${Object.values(domainMap).join(', ')}`;
    }

    const boost = await apiCall(COACH_PORT, 'POST', '/api/v1/auto-coach/boost', {
      rounds: 10,
      focus: domain
    });

    return `âœ… Focused Training on ${fullDomain}\n\n` +
           `Current Mastery: ${current.mastery}%\n` +
           `Training Rounds: ${boost.boosted}\n` +
           `Sticky Batch Mode: ON (keeps focus on this topic)`;
  } catch (e) {
    return `Error: ${e.message}`;
  }
}

async function optimize() {
  try {
    const meta = await apiCall(META_PORT, 'POST', '/api/v4/meta-learning/boost-retention', {
      retentionDelta: 0.15,
      transferDelta: 0.10
    });

    if (!meta.ok) {
      return "Meta-learning boost failed.";
    }

    return `âœ… Meta-Learning Optimization Complete\n\n` +
           `Retention Boost: +15%\n` +
           `Transfer Learning: +10%\n\n` +
           `What this means:\n` +
           `  â€¢ Better long-term memory of what you learned\n` +
           `  â€¢ Knowledge transfers to related topics\n` +
           `  â€¢ Reduced forgetting curve`;
  } catch (e) {
    return `Error: ${e.message}`;
  }
}

function help() {
  return `ðŸ¤– TooLoo AI Coach\n\n` +
         `I'm your personalized learning coach powered by AI. I can:\n\n` +
         `â€¢ Analyze your learning progress\n` +
         `â€¢ Provide personalized coaching advice\n` +
         `â€¢ Create focused training sessions\n` +
         `â€¢ Optimize your learning path\n\n` +
         `Try asking:\n` +
         `â€¢ "How am I doing?"\n` +
         `â€¢ "What should I focus on?"\n` +
         `â€¢ "I'm struggling with distributed systems"\n` +
         `â€¢ "Coach me on databases"\n` +
         `â€¢ "How can I improve faster?"`;
}
