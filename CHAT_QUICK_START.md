# ðŸš€ TooLoo.ai Chat â€” Quick Start

## Access the Chat

**Live Chat Interface:**
```
http://localhost:3000/chat-modern
```

Open in your browser to chat with Claude Haiku 4.5 (with intelligent fallback to GPT-4/DeepSeek).

---

## Current System Status

### Services Running
- âœ… **Web Server** (Port 3000) â€” Serving chat UI
- âœ… **API Bridge** (Port 3010) â€” Multi-provider router
- âœ… **Claude Haiku 4.5** â€” Primary AI model
- âœ… **Fallback Chain** â€” OpenAI/DeepSeek if primary fails

### Performance
- **Latency:** 2.5 seconds average
- **Grade:** C (Fair)
- **Success Rate:** 100%
- **Cost:** $0.000314/request

---

## What's Happening Under the Hood

### Intelligent Routing
When you ask a question:

1. **Detect Task Type** â€” Is this reasoning? Coding? Creative? Analysis?
2. **Route Intelligently** â€” Pick best provider:
   - Reasoning â†’ Claude Haiku 4.5 âš¡
   - Coding â†’ Claude Haiku 4.5 âš¡
   - Creative â†’ (OpenAI) â†’ fallback to Claude
   - Analysis â†’ (DeepSeek) â†’ fallback to Claude

3. **Fallback Chain** â€” If provider fails, automatically retry with next
4. **Respond** â€” Stream response back to UI in ~2-3 seconds

### Example Queries

**Test Reasoning (Haiku's Strength):**
```
"If all roses are flowers and some flowers are red, what can we conclude about roses?"
```
Expected: ~2s, good logical response

**Test Coding (Haiku's Strength):**
```
"Write a function to reverse a string in JavaScript"
```
Expected: ~3-4s, concise code

**Test Creative (Fallback):**
```
"Write a vivid description of a sunset"
```
Expected: ~2s (fallback), poetic response

---

## API Endpoints (For Developers)

### Chat Message
```
POST http://localhost:3010/api/v1/chat/message
Content-Type: application/json

{
  "message": "What is 2+2?",
  "conversationHistory": []
}

Response:
{
  "response": "2+2 equals 4.",
  "provider": "Claude 3.5 Haiku",
  "taskType": "reasoning",
  "tokens": 8
}
```

### Provider Status
```
GET http://localhost:3010/api/v1/system/status

Response:
{
  "status": "healthy",
  "providers": {
    "anthropic": "available",
    "openai": "available",
    "deepseek": "available",
    "gemini": "not configured",
    "ollama": "available (local)"
  },
  "activeProvider": "Claude 3.5 Haiku"
}
```

### Health Check
```
GET http://localhost:3010/health

Response:
{
  "ok": true,
  "service": "chat-api",
  "port": 3010
}
```

---

## Monitoring Performance

### Run Latency Benchmark
```bash
cd /workspaces/TooLoo.ai
node latency-benchmark.js
```

Shows:
- Average latency for 4 test tasks
- Token usage and cost
- Performance grade (A+, A, B+, B, C+, C, D, F)
- Optimization recommendations

### Check Logs
```bash
tail -f /tmp/chat-api.log    # API bridge logs
tail -f /tmp/web-server.log  # Web server logs
```

---

## Troubleshooting

### Chat Returns Error
1. Check bridge is running: `curl http://localhost:3010/health`
2. Check logs: `tail -20 /tmp/chat-api.log`
3. Verify API keys are set: `echo $ANTHROPIC_API_KEY`
4. Restart bridge:
   ```bash
   pkill -f "node servers/chat-api-bridge"
   export ANTHROPIC_API_KEY="your-key"
   node servers/chat-api-bridge.js &
   ```

### High Latency (>5s)
1. Check if OpenAI API is working (fallback might be slow)
2. Check network connectivity
3. Consider stopping other CPU-intensive processes

### Timeout Errors
1. Bridge might be overloaded
2. API provider might be down
3. Try again in 30 seconds

---

## Performance Stats

| Metric | Value | Target |
|--------|-------|--------|
| Avg Latency | 2522ms | <2000ms (B grade) |
| Success Rate | 100% | 100% âœ… |
| Avg Tokens | 393 | <300 (optimize further) |
| Cost/Request | $0.000314 | <$0.0002 |
| Grade | C | B (next target) |

---

## Next Improvements (Roadmap)

- [ ] Streaming responses (15% faster perceived latency)
- [ ] Response caching (80% faster for repeats)
- [ ] Real-time monitoring dashboard
- [ ] Auto-scaling for high traffic
- [ ] A/B testing for prompt optimization

---

**Status:** âœ… Ready for production deployment  
**Last Updated:** October 21, 2025
