# TooLoo.ai Synapsys - Docker Compose Configuration
# Version: 3.3.350
#
# Production deployment with optional services:
# - tooloo: Main application
# - ollama: Local AI backend (optional)
# - redis: Caching layer (optional, for future scaling)

version: '3.8'

services:
  # ===========================================================================
  # Main TooLoo Application
  # ===========================================================================
  tooloo:
    build:
      context: .
      dockerfile: Dockerfile
      target: runner
    container_name: tooloo-synapsys
    restart: unless-stopped
    ports:
      - "${PORT:-4000}:4000"
      - "${VITE_PORT:-5173}:5173"
    environment:
      - NODE_ENV=${NODE_ENV:-production}
      - PORT=4000
      - DATA_DIR=/app/data
      # AI Provider API Keys (passed from host)
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      # LocalAI/Ollama connection
      - LOCALAI_BASE_URL=${LOCALAI_BASE_URL:-http://ollama:11434}
      - ENABLE_LOCALAI=${ENABLE_LOCALAI:-false}
      # Encryption (generated on first run if not set)
      - ENCRYPTION_KEY=${ENCRYPTION_KEY:-}
      - ENCRYPTION_SALT=${ENCRYPTION_SALT:-}
    volumes:
      - tooloo-data:/app/data
      - tooloo-logs:/app/logs
      - tooloo-temp:/app/temp
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - tooloo-network
    depends_on:
      ollama:
        condition: service_healthy
        required: false

  # ===========================================================================
  # Ollama - Local AI Backend (Optional)
  # ===========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: tooloo-ollama
    restart: unless-stopped
    profiles:
      - localai
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama-models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - tooloo-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # ===========================================================================
  # Redis - Caching Layer (Optional, for scaling)
  # ===========================================================================
  redis:
    image: redis:7-alpine
    container_name: tooloo-redis
    restart: unless-stopped
    profiles:
      - cache
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - tooloo-network

# =============================================================================
# Networks
# =============================================================================
networks:
  tooloo-network:
    driver: bridge

# =============================================================================
# Volumes
# =============================================================================
volumes:
  tooloo-data:
    driver: local
  tooloo-logs:
    driver: local
  tooloo-temp:
    driver: local
  ollama-models:
    driver: local
  redis-data:
    driver: local
